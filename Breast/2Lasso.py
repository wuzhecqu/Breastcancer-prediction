import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import Lasso, LassoCV
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression
import joblib
import os

# è®¾ç½®æ ·å¼
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

# 1. åŠ è½½è®­ç»ƒæ•°æ®
train_path = r'F:\Project\Breast\train_data.csv'
train_df = pd.read_csv(train_path)

print("=" * 60)
print("è®­ç»ƒæ•°æ®åŸºæœ¬ä¿¡æ¯:")
print("=" * 60)
print(f"è®­ç»ƒæ•°æ®å½¢çŠ¶: {train_df.shape}")

# 2. åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡å˜é‡
X_train = train_df.iloc[:, :-2]  # ç‰¹å¾ï¼ˆå‰30åˆ—ï¼‰
y_train = train_df['diagnosis']  # ç¼–ç åçš„æ ‡ç­¾ï¼ˆ0=è‰¯æ€§, 1=æ¶æ€§ï¼‰

print(f"ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {X_train.shape}")
print(f"ç›®æ ‡å˜é‡å½¢çŠ¶: {y_train.shape}")

# 3. æ ‡å‡†åŒ–ç‰¹å¾
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
feature_names = X_train.columns.tolist()

# 4. ä½¿ç”¨LassoCVé€‰æ‹©æœ€ä½³alpha
print("\n" + "=" * 60)
print("ä½¿ç”¨LassoCVé€‰æ‹©æœ€ä½³æ­£åˆ™åŒ–å‚æ•°alpha...")
print("=" * 60)

alphas = np.logspace(-4, 0, 50)
lasso_cv = LassoCV(alphas=alphas, cv=5, max_iter=10000, random_state=42)
lasso_cv.fit(X_train_scaled, y_train)

print(f"æœ€ä½³alphaå€¼: {lasso_cv.alpha_:.6f}")

# 5. ä½¿ç”¨æœ€ä½³alphaè®­ç»ƒLassoæ¨¡å‹
lasso_best = Lasso(alpha=lasso_cv.alpha_, max_iter=10000, random_state=42)
lasso_best.fit(X_train_scaled, y_train)

# è·å–ç³»æ•°
coef = lasso_best.coef_

# 6. ç­›é€‰ç³»æ•°ç»å¯¹å€¼å¤§äº0.1çš„ç‰¹å¾
print("\n" + "=" * 60)
print("ç­›é€‰ç³»æ•°ç»å¯¹å€¼ > 0.1 çš„ç‰¹å¾:")
print("=" * 60)

# åˆ›å»ºç³»æ•°DataFrame
coef_df = pd.DataFrame({
    'feature': feature_names,
    'coefficient': coef,
    'abs_coefficient': np.abs(coef)
})

# ç­›é€‰ç³»æ•°ç»å¯¹å€¼å¤§äº0.1çš„ç‰¹å¾
selected_features_df = coef_df[coef_df['abs_coefficient'] > 0.1].copy()
selected_features_df = selected_features_df.sort_values('abs_coefficient', ascending=False)

# æŒ‰ç³»æ•°å€¼æ’åºï¼ˆæ­£è´Ÿåˆ†å¼€æ˜¾ç¤ºï¼‰
selected_features_pos = selected_features_df[selected_features_df['coefficient'] > 0].sort_values('coefficient', ascending=False)
selected_features_neg = selected_features_df[selected_features_df['coefficient'] < 0].sort_values('coefficient')

print(f"\næ‰¾åˆ° {len(selected_features_df)} ä¸ªç³»æ•°ç»å¯¹å€¼å¤§äº0.1çš„ç‰¹å¾:")
print(f"æ­£ç³»æ•°ç‰¹å¾ ({len(selected_features_pos)}ä¸ª): ä¸æ¶æ€§æ­£ç›¸å…³")
print(f"è´Ÿç³»æ•°ç‰¹å¾ ({len(selected_features_neg)}ä¸ª): ä¸æ¶æ€§è´Ÿç›¸å…³ï¼ˆä¸è‰¯æ€§æ­£ç›¸å…³ï¼‰")

print("\n" + "-" * 60)
print("æ­£ç³»æ•°ç‰¹å¾ï¼ˆå€¼è¶Šå¤§è¶Šå¯èƒ½æ˜¯æ¶æ€§ï¼‰:")
print("-" * 60)
for i, (idx, row) in enumerate(selected_features_pos.iterrows(), 1):
    print(f"{i:2d}. {row['feature']:30s} ç³»æ•°: {row['coefficient']:+.6f}")

print("\n" + "-" * 60)
print("è´Ÿç³»æ•°ç‰¹å¾ï¼ˆå€¼è¶Šå¤§è¶Šå¯èƒ½æ˜¯è‰¯æ€§ï¼‰:")
print("-" * 60)
for i, (idx, row) in enumerate(selected_features_neg.iterrows(), 1):
    print(f"{i:2d}. {row['feature']:30s} ç³»æ•°: {row['coefficient']:+.6f}")

# 7. è·å–é€‰ä¸­çš„ç‰¹å¾åç§°
selected_features = selected_features_df['feature'].tolist()

print(f"\n" + "=" * 60)
print("ç‰¹å¾é€‰æ‹©ç»Ÿè®¡:")
print("=" * 60)
print(f"åŸå§‹ç‰¹å¾æ€»æ•°: {len(feature_names)}")
print(f"é€‰ä¸­ç‰¹å¾æ•°é‡: {len(selected_features)}")
print(f"ç‰¹å¾å‡å°‘æ¯”ä¾‹: {(1 - len(selected_features)/len(feature_names)):.1%}")
print(f"ä¿ç•™çš„ç‰¹å¾æ¯”ä¾‹: {len(selected_features)/len(feature_names):.1%}")

# 8. åˆ›å»ºé€‰ä¸­ç‰¹å¾çš„æ•°æ®é›†
X_train_selected = X_train[selected_features]
X_train_selected_scaled = X_train_scaled[:, [feature_names.index(f) for f in selected_features]]

# 9. éªŒè¯é€‰ä¸­ç‰¹å¾çš„æ•ˆæœ
print("\n" + "=" * 60)
print("ç‰¹å¾é€‰æ‹©æ•ˆæœéªŒè¯:")
print("=" * 60)

# ä½¿ç”¨é€»è¾‘å›å½’è¿›è¡ŒéªŒè¯
logreg_full = LogisticRegression(max_iter=1000, random_state=42)
scores_full = cross_val_score(logreg_full, X_train_scaled, y_train, cv=5, scoring='accuracy')
print(f"å…¨éƒ¨ç‰¹å¾ ({len(feature_names)}ä¸ª) çš„äº¤å‰éªŒè¯å‡†ç¡®ç‡:")
print(f"  å¹³å‡: {scores_full.mean():.4f}")
print(f"  æ ‡å‡†å·®: {scores_full.std():.4f}")

logreg_selected = LogisticRegression(max_iter=1000, random_state=42)
scores_selected = cross_val_score(logreg_selected, X_train_selected_scaled, y_train, cv=5, scoring='accuracy')
print(f"\né€‰ä¸­ç‰¹å¾ ({len(selected_features)}ä¸ª) çš„äº¤å‰éªŒè¯å‡†ç¡®ç‡:")
print(f"  å¹³å‡: {scores_selected.mean():.4f}")
print(f"  æ ‡å‡†å·®: {scores_selected.std():.4f}")

# è®¡ç®—æ€§èƒ½å˜åŒ–
accuracy_change = (scores_selected.mean() - scores_full.mean()) / scores_full.mean() * 100
print(f"\nå‡†ç¡®ç‡å˜åŒ–: {accuracy_change:+.2f}%")

if accuracy_change > 0:
    print("âœ“ ç‰¹å¾é€‰æ‹©æå‡äº†æ¨¡å‹æ€§èƒ½ï¼")
elif abs(accuracy_change) < 2:
    print("â—‹ ç‰¹å¾é€‰æ‹©å¯¹æ€§èƒ½å½±å“ä¸å¤§ï¼Œä½†ç®€åŒ–äº†æ¨¡å‹ã€‚")
else:
    print("âš  ç‰¹å¾é€‰æ‹©é™ä½äº†æ¨¡å‹æ€§èƒ½ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´é˜ˆå€¼ã€‚")

# 10. å¯è§†åŒ–ç‰¹å¾ç³»æ•°
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# å­å›¾1ï¼šé€‰ä¸­ç‰¹å¾çš„ç³»æ•°ï¼ˆæŒ‰å€¼æ’åºï¼‰
ax1 = axes[0, 0]
all_selected_features = pd.concat([selected_features_pos, selected_features_neg])
colors = ['red' if c < 0 else 'blue' for c in all_selected_features['coefficient']]
bars = ax1.barh(range(len(all_selected_features)), all_selected_features['coefficient'], color=colors)
ax1.set_yticks(range(len(all_selected_features)))
ax1.set_yticklabels(all_selected_features['feature'])
ax1.set_xlabel('Lassoç³»æ•°å€¼')
ax1.set_title(f'é€‰ä¸­ç‰¹å¾ç³»æ•° (ç»å¯¹å€¼ > 0.1, å…±{len(all_selected_features)}ä¸ª)')
ax1.axvline(x=0, color='black', linewidth=0.8)

# æ·»åŠ ç³»æ•°å€¼æ ‡ç­¾
for i, (bar, coeff) in enumerate(zip(bars, all_selected_features['coefficient'])):
    ax1.text(coeff + (0.01 if coeff >= 0 else -0.03), bar.get_y() + bar.get_height()/2,
             f'{coeff:.3f}', ha='left' if coeff >= 0 else 'right', va='center', fontsize=9)

# å­å›¾2ï¼šç³»æ•°ç»å¯¹å€¼åˆ†å¸ƒ
ax2 = axes[0, 1]
sorted_by_abs = selected_features_df.sort_values('abs_coefficient', ascending=True)
ax2.barh(range(len(sorted_by_abs)), sorted_by_abs['abs_coefficient'], color='green')
ax2.set_yticks(range(len(sorted_by_abs)))
ax2.set_yticklabels(sorted_by_abs['feature'])
ax2.set_xlabel('ç³»æ•°ç»å¯¹å€¼')
ax2.set_title('ç‰¹å¾é‡è¦æ€§æ’åºï¼ˆæŒ‰ç³»æ•°ç»å¯¹å€¼ï¼‰')
ax2.axvline(x=0.1, color='red', linestyle='--', linewidth=1.5, label='é˜ˆå€¼=0.1')

# æ·»åŠ é˜ˆå€¼çº¿è¯´æ˜
ax2.text(0.1 + 0.01, len(sorted_by_abs)/2, f'é˜ˆå€¼çº¿\n(>0.1ä¿ç•™)',
         verticalalignment='center', color='red', fontweight='bold')

# å­å›¾3ï¼šç‰¹å¾æ•°é‡å¯¹æ¯”
ax3 = axes[1, 0]
categories = ['åŸå§‹ç‰¹å¾', 'é€‰ä¸­ç‰¹å¾']
counts = [len(feature_names), len(selected_features)]
colors_comp = ['lightblue', 'lightgreen']
bars3 = ax3.bar(categories, counts, color=colors_comp)
ax3.set_ylabel('ç‰¹å¾æ•°é‡')
ax3.set_title('ç‰¹å¾é€‰æ‹©å‰åæ•°é‡å¯¹æ¯”')
ax3.grid(True, alpha=0.3, axis='y')

# åœ¨æŸ±çŠ¶å›¾ä¸Šæ˜¾ç¤ºæ•°é‡
for bar, count in zip(bars3, counts):
    height = bar.get_height()
    ax3.text(bar.get_x() + bar.get_width()/2, height + 0.5,
             str(count), ha='center', va='bottom', fontweight='bold')

# å­å›¾4ï¼šæ¨¡å‹æ€§èƒ½å¯¹æ¯”
ax4 = axes[1, 1]
x_pos = [0, 1]
full_mean = scores_full.mean()
full_std = scores_full.std()
selected_mean = scores_selected.mean()
selected_std = scores_selected.std()

bars4 = ax4.bar(x_pos, [full_mean, selected_mean], yerr=[full_std, selected_std],
               capsize=10, color=['lightcoral', 'lightgreen'], alpha=0.7)
ax4.set_xticks(x_pos)
ax4.set_xticklabels(['å…¨éƒ¨ç‰¹å¾', 'é€‰ä¸­ç‰¹å¾'])
ax4.set_ylabel('äº¤å‰éªŒè¯å‡†ç¡®ç‡')
ax4.set_title('æ¨¡å‹æ€§èƒ½å¯¹æ¯”')
ax4.set_ylim([0.9, 1.0])
ax4.grid(True, alpha=0.3, axis='y')

# åœ¨æŸ±çŠ¶å›¾ä¸Šæ˜¾ç¤ºå‡†ç¡®ç‡
for bar, mean, std in zip(bars4, [full_mean, selected_mean], [full_std, selected_std]):
    height = bar.get_height()
    ax4.text(bar.get_x() + bar.get_width()/2, height + 0.002,
             f'{mean:.4f}\nÂ±{std:.4f}', ha='center', va='bottom', fontsize=9)

plt.tight_layout()
plt.savefig(r'F:\Project\Breast\lasso_features_gt_0.1.png', dpi=300, bbox_inches='tight')
plt.show()

# 11. ä¿å­˜ç»“æœ
output_dir = r'F:\Project\Breast\feature_selection_lasso_0.1'
os.makedirs(output_dir, exist_ok=True)

# ä¿å­˜é€‰ä¸­ç‰¹å¾åˆ—è¡¨
selected_features_df.to_csv(os.path.join(output_dir, 'selected_features_gt_0.1.csv'), index=False)

# ä¿å­˜æ‰€æœ‰ç‰¹å¾çš„ç³»æ•°ï¼ˆæ ‡è®°æ˜¯å¦é€‰ä¸­ï¼‰
coef_df['selected'] = coef_df['abs_coefficient'] > 0.1
coef_df_sorted = coef_df.sort_values('abs_coefficient', ascending=False)
coef_df_sorted.to_csv(os.path.join(output_dir, 'all_features_with_selection.csv'), index=False)

# ä¿å­˜é€‰ä¸­ç‰¹å¾çš„æ•°æ®é›†
train_selected_df = pd.DataFrame(X_train_selected)
train_selected_df['diagnosis'] = y_train.values
train_selected_df['diagnosis_original'] = train_df['diagnosis_original'].values
train_selected_df.to_csv(os.path.join(output_dir, 'train_data_selected.csv'), index=False)

# ä¿å­˜æ ‡å‡†åŒ–å™¨
joblib.dump(scaler, os.path.join(output_dir, 'scaler.pkl'))

# ä¿å­˜ç‰¹å¾ç´¢å¼•æ˜ å°„
feature_mapping = {
    'feature_names': feature_names,
    'selected_features': selected_features,
    'selected_indices': [feature_names.index(f) for f in selected_features]
}
import json
with open(os.path.join(output_dir, 'feature_mapping.json'), 'w') as f:
    json.dump(feature_mapping, f, indent=2)

print(f"\n" + "=" * 60)
print("ç»“æœä¿å­˜:")
print("=" * 60)
print(f"1. é€‰ä¸­ç‰¹å¾åˆ—è¡¨: {output_dir}\\selected_features_gt_0.1.csv")
print(f"2. æ‰€æœ‰ç‰¹å¾ç³»æ•°: {output_dir}\\all_features_with_selection.csv")
print(f"3. é€‰ä¸­ç‰¹å¾æ•°æ®é›†: {output_dir}\\train_data_selected.csv")
print(f"4. æ ‡å‡†åŒ–å™¨: {output_dir}\\scaler.pkl")
print(f"5. ç‰¹å¾æ˜ å°„æ–‡ä»¶: {output_dir}\\feature_mapping.json")
print(f"6. å¯è§†åŒ–å›¾è¡¨: F:\\Project\\Breast\\lasso_features_gt_0.1.png")

# 12. ç”Ÿæˆç‰¹å¾é€‰æ‹©æŠ¥å‘Š
print("\n" + "=" * 60)
print("ç‰¹å¾é€‰æ‹©æŠ¥å‘Š:")
print("=" * 60)

print("\nğŸ” æœ€é‡è¦çš„5ä¸ªç‰¹å¾ï¼ˆæŒ‰ç»å¯¹å€¼ï¼‰:")
top5 = selected_features_df.head(5)
for i, (idx, row) in enumerate(top5.iterrows(), 1):
    direction = "æ­£ç›¸å…³" if row['coefficient'] > 0 else "è´Ÿç›¸å…³"
    print(f"{i}. {row['feature']}")
    print(f"   ç³»æ•°: {row['coefficient']:.4f} ({direction})")
    print(f"   ç»å¯¹å€¼: {row['abs_coefficient']:.4f}")

print(f"\nğŸ“Š è¢«æ’é™¤çš„ç‰¹å¾ ({len(feature_names) - len(selected_features)}ä¸ª):")
excluded_features = coef_df[~coef_df['selected']]['feature'].tolist()
if excluded_features:
    # åˆ†ç»„æ˜¾ç¤º
    for i in range(0, len(excluded_features), 5):
        print("   " + ", ".join(excluded_features[i:i+5]))

print("\nğŸ’¡ ç‰¹å¾å«ä¹‰è§£é‡Š:")
print("   - æ­£ç³»æ•°ç‰¹å¾: å€¼è¶Šå¤§ï¼Œè¶Šå¯èƒ½ä¸ºæ¶æ€§(M)")
print("   - è´Ÿç³»æ•°ç‰¹å¾: å€¼è¶Šå¤§ï¼Œè¶Šå¯èƒ½ä¸ºè‰¯æ€§(B)")

print("\n" + "=" * 60)
print("Lassoç‰¹å¾é€‰æ‹©å®Œæˆï¼ç³»æ•°ç»å¯¹å€¼>0.1çš„ç‰¹å¾å·²ç­›é€‰ã€‚")
print("=" * 60)

# 13. ä½¿ç”¨å»ºè®®
print("\nğŸ“‹ åç»­ä½¿ç”¨å»ºè®®:")
print("1. å¯¹éªŒè¯é›†åº”ç”¨ç›¸åŒçš„ç‰¹å¾é€‰æ‹©:")
print("   ```python")
print("   # åŠ è½½éªŒè¯é›†")
print("   val_df = pd.read_csv(r'F:\\Project\\Breast\\splitted_data\\validation_data.csv')")
print("   X_val = val_df.iloc[:, :-2]")
print("   y_val = val_df['diagnosis']")
print("   ")
print("   # ä½¿ç”¨ç›¸åŒçš„æ ‡å‡†åŒ–å™¨")
print("   scaler = joblib.load(r'F:\\Project\\Breast\\feature_selection_lasso_0.1\\scaler.pkl')")
print("   X_val_scaled = scaler.transform(X_val)")
print("   ")
print("   # åªé€‰æ‹©ç›¸åŒçš„ç‰¹å¾")
print("   selected_features = pd.read_csv(r'F:\\Project\\Breast\\feature_selection_lasso_0.1\\selected_features_gt_0.1.csv')")
print("   selected_features_list = selected_features['feature'].tolist()")
print("   X_val_selected = X_val[selected_features_list]")
print("   X_val_selected_scaled = X_val_scaled[:, [feature_names.index(f) for f in selected_features_list]]")
print("   ```")